Параллельный Веб-Краулер

Проект — многопроцессный веб-краулер, который обходил сайт с ограничением глубины и количества страниц, используя очередь задач и пул процессов.

Особенности

- Параллельная обработка страниц с помощью `multiprocessing`
- Ограничение по глубине обхода и количеству страниц
- Фильтрация ссылок по домену
- Логирование с ротацией файлов
- Консольный прогресс-бар
- Легко расширяемый и модульный код

Установка

bash
pip install -r requirements.txt
